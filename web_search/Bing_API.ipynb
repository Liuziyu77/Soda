{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "input your Bing API and test web search\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1375,
     "status": "ok",
     "timestamp": 1712481598835,
     "user": {
      "displayName": "åˆ˜æŸ",
      "userId": "17997477929294889377"
     },
     "user_tz": -480
    },
    "id": "o56ILroftBdi"
   },
   "outputs": [],
   "source": [
    "subscription_key = \"********************************\"\n",
    "assert subscription_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 315,
     "status": "ok",
     "timestamp": 1712481618047,
     "user": {
      "displayName": "åˆ˜æŸ",
      "userId": "17997477929294889377"
     },
     "user_tz": -480
    },
    "id": "pf8pIZzVulyN"
   },
   "outputs": [],
   "source": [
    "search_url = \"https://api.bing.microsoft.com/v7.0/search\"\n",
    "search_term = \"LLaVa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 524,
     "status": "ok",
     "timestamp": 1712481620257,
     "user": {
      "displayName": "åˆ˜æŸ",
      "userId": "17997477929294889377"
     },
     "user_tz": -480
    },
    "id": "-3Cmm9Gf23fr"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\"Ocp-Apim-Subscription-Key\": subscription_key}\n",
    "params = {\"q\": search_term, \"textDecorations\": True, \"textFormat\": \"HTML\"}\n",
    "response = requests.get(search_url, headers=headers, params=params)\n",
    "response.raise_for_status()\n",
    "search_results = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "executionInfo": {
     "elapsed": 316,
     "status": "ok",
     "timestamp": 1712481635040,
     "user": {
      "displayName": "åˆ˜æŸ",
      "userId": "17997477929294889377"
     },
     "user_tz": -480
    },
    "id": "DcXJq5lf3AKc",
    "outputId": "6e8c8bdc-c371-4139-b6c7-18a01a5c5b5a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr>\n",
       "                       <td><a href=\"https://github.com/haotian-liu/LLaVA\">haotian-liu/LLaVA: [NeurIPS&#39;23 Oral] Visual Instruction Tuning (LLaVA) built towards GPT-4V level capabilities and beyond. - GitHub</a></td>\n",
       "                       <td>[10/26] ğŸ”¥ <b>LLaVA</b>-1.5 with LoRA achieves comparable performance as full-model finetuning, with a reduced GPU RAM requirement (ckpts, script). We also provide a doc on how to finetune <b>LLaVA</b>-1.5 on your own dataset with LoRA. [10/12] Check out the Korean]</td>\n",
       "                     </tr>\n",
       "<tr>\n",
       "                       <td><a href=\"https://llava-vl.github.io/\">LLaVA</a></td>\n",
       "                       <td><b>LLaVA</b> represents a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding, achieving impressive chat capabilities mimicking spirits of the multimodal GPT-4 and setting a new state-of-the-art accuracy on Science QA.</td>\n",
       "                     </tr>\n",
       "<tr>\n",
       "                       <td><a href=\"https://llava.hliu.cc/\">LLaVA</a></td>\n",
       "                       <td>Image. Drop Image Here - or - Click to Upload. Examples. What is unusual about this image? What are the things I should be cautious about when I visit here? Parameters . <b>LLaVA</b> Chatbot. ğŸ‘ Upvote.</td>\n",
       "                     </tr>\n",
       "<tr>\n",
       "                       <td><a href=\"https://zhuanlan.zhihu.com/p/624928279\">LLaVAï¼ˆLarge Language and Vision Assistantï¼‰å¤§æ¨¡å‹ - çŸ¥ä¹</a></td>\n",
       "                       <td><b>LLaVA</b>ï¼ˆLarge Language and Vision Assistantï¼‰æ˜¯ä¸€ä¸ªç”±å¨æ–¯åº·æ˜Ÿå¤§å­¦éº¦è¿ªé€Šåˆ†æ ¡ã€å¾®è½¯ç ”ç©¶é™¢å’Œå“¥ä¼¦æ¯”äºšå¤§å­¦ç ”ç©¶è€…å…±åŒå‘å¸ƒçš„å¤šæ¨¡æ€å¤§æ¨¡å‹ã€‚. è¯¥æ¨¡å‹å±•ç¤ºå‡ºäº†ä¸€äº›æ¥è¿‘å¤šæ¨¡æ€ GPT-4 çš„å›¾æ–‡ç†è§£èƒ½åŠ›ï¼šç›¸å¯¹äº GPT-4 è·å¾—äº† 85.1% çš„ç›¸å¯¹å¾—åˆ†ã€‚. å½“åœ¨ç§‘å­¦é—®ç­”ï¼ˆScience QAï¼‰ä¸Šè¿›è¡Œ ...</td>\n",
       "                     </tr>\n",
       "<tr>\n",
       "                       <td><a href=\"https://llava-vl.github.io/blog/2024-01-30-llava-1-6/\">LLaVA-1.6: Improved reasoning, OCR, and world knowledge</a></td>\n",
       "                       <td>Today, we are thrilled to present <b>LLaVA</b>-1.6, with improved reasoning, OCR, and world knowledge. <b>LLaVA</b>-1.6 even exceeds Gemini Pro on several benchmarks. Compared with <b>LLaVA</b>-1.5, <b>LLaVA</b>-1.6 has several improvements: Increasing the input image to 4x</td>\n",
       "                     </tr>\n",
       "<tr>\n",
       "                       <td><a href=\"https://www.microsoft.com/en-us/research/project/llava-large-language-and-vision-assistant/\">LLaVA: Large Language and Vision Assistant - Microsoft Research</a></td>\n",
       "                       <td><b>LLaVA</b> is an open-source project, collaborating with research community to advance the state-of-the-art in AI. <b>LLaVA</b> represents the first end-to-end trained large multimodal model (LMM) that achieves impressive chat capabilities mimicking spirits of the multimodal GPT-4.</td>\n",
       "                     </tr>\n",
       "<tr>\n",
       "                       <td><a href=\"https://llava-vl.github.io/llava-interactive/\">LLaVA-Interactive</a></td>\n",
       "                       <td><b>LLaVA</b>-Interactive is a system-level synergy of the inference stages of three models, without additional model training. It is surprisingly cheap to build. Checkout our code release on GitHub. For better demo experience, please play <b>LLaVA</b>-Interactive in a seperate tab by clicking me.</td>\n",
       "                     </tr>\n",
       "<tr>\n",
       "                       <td><a href=\"https://arxiv.org/abs/2304.08485\">[2304.08485] Visual Instruction Tuning - arXiv.org</a></td>\n",
       "                       <td>By instruction tuning on such generated data, we introduce <b>LLaVA</b>: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early</td>\n",
       "                     </tr>\n",
       "<tr>\n",
       "                       <td><a href=\"https://github.com/haotian-liu/LLaVA/releases\">Releases Â· haotian-liu/LLaVA Â· GitHub</a></td>\n",
       "                       <td>Release v1.1.0. ğŸ”¥ <b>LLaVA</b>-1.5 is out! This release supports <b>LLaVA</b>-1.5 model inference and serving. We will release the training scripts, data, and evaluation scripts on benchmarks in the coming week.</td>\n",
       "                     </tr>\n",
       "<tr>\n",
       "                       <td><a href=\"https://github.com/haotian-liu/LLaVA/blob/main/README.md\">LLaVA/README.md at main Â· haotian-liu/LLaVA Â· GitHub</a></td>\n",
       "                       <td>[NeurIPS&#39;23 Oral] Visual Instruction Tuning (<b>LLaVA</b>) built towards GPT-4V level capabilities and beyond. - <b>LLaVA</b>/README.md at main Â· haotian-liu/<b>LLaVA</b></td>\n",
       "                     </tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "rows = \"\\n\".join([\"\"\"<tr>\n",
    "                       <td><a href=\\\"{0}\\\">{1}</a></td>\n",
    "                       <td>{2}</td>\n",
    "                     </tr>\"\"\".format(v[\"url\"], v[\"name\"], v[\"snippet\"])\n",
    "                  for v in search_results[\"webPages\"][\"value\"]])\n",
    "HTML(\"<table>{0}</table>\".format(rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNg5Jz6VfBt+1bKMOC2VI7z",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
