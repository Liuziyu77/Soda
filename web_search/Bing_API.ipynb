{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "input your Bing API and test web search\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1375,
     "status": "ok",
     "timestamp": 1712481598835,
     "user": {
      "displayName": "刘某",
      "userId": "17997477929294889377"
     },
     "user_tz": -480
    },
    "id": "o56ILroftBdi"
   },
   "outputs": [],
   "source": [
    "subscription_key = \"********************************\"\n",
    "assert subscription_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 315,
     "status": "ok",
     "timestamp": 1712481618047,
     "user": {
      "displayName": "刘某",
      "userId": "17997477929294889377"
     },
     "user_tz": -480
    },
    "id": "pf8pIZzVulyN"
   },
   "outputs": [],
   "source": [
    "search_url = \"https://api.bing.microsoft.com/v7.0/search\"\n",
    "search_term = \"LLaVa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 524,
     "status": "ok",
     "timestamp": 1712481620257,
     "user": {
      "displayName": "刘某",
      "userId": "17997477929294889377"
     },
     "user_tz": -480
    },
    "id": "-3Cmm9Gf23fr"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\"Ocp-Apim-Subscription-Key\": subscription_key}\n",
    "params = {\"q\": search_term, \"textDecorations\": True, \"textFormat\": \"HTML\"}\n",
    "response = requests.get(search_url, headers=headers, params=params)\n",
    "response.raise_for_status()\n",
    "search_results = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "executionInfo": {
     "elapsed": 316,
     "status": "ok",
     "timestamp": 1712481635040,
     "user": {
      "displayName": "刘某",
      "userId": "17997477929294889377"
     },
     "user_tz": -480
    },
    "id": "DcXJq5lf3AKc",
    "outputId": "6e8c8bdc-c371-4139-b6c7-18a01a5c5b5a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr>\n",
       "                       <td><a href=\"https://github.com/haotian-liu/LLaVA\">haotian-liu/LLaVA: [NeurIPS&#39;23 Oral] Visual Instruction Tuning (LLaVA) built towards GPT-4V level capabilities and beyond. - GitHub</a></td>\n",
       "                       <td>[10/26] 🔥 <b>LLaVA</b>-1.5 with LoRA achieves comparable performance as full-model finetuning, with a reduced GPU RAM requirement (ckpts, script). We also provide a doc on how to finetune <b>LLaVA</b>-1.5 on your own dataset with LoRA. [10/12] Check out the Korean]</td>\n",
       "                     </tr>\n",
       "<tr>\n",
       "                       <td><a href=\"https://llava-vl.github.io/\">LLaVA</a></td>\n",
       "                       <td><b>LLaVA</b> represents a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding, achieving impressive chat capabilities mimicking spirits of the multimodal GPT-4 and setting a new state-of-the-art accuracy on Science QA.</td>\n",
       "                     </tr>\n",
       "<tr>\n",
       "                       <td><a href=\"https://llava.hliu.cc/\">LLaVA</a></td>\n",
       "                       <td>Image. Drop Image Here - or - Click to Upload. Examples. What is unusual about this image? What are the things I should be cautious about when I visit here? Parameters . <b>LLaVA</b> Chatbot. 👍 Upvote.</td>\n",
       "                     </tr>\n",
       "<tr>\n",
       "                       <td><a href=\"https://zhuanlan.zhihu.com/p/624928279\">LLaVA（Large Language and Vision Assistant）大模型 - 知乎</a></td>\n",
       "                       <td><b>LLaVA</b>（Large Language and Vision Assistant）是一个由威斯康星大学麦迪逊分校、微软研究院和哥伦比亚大学研究者共同发布的多模态大模型。. 该模型展示出了一些接近多模态 GPT-4 的图文理解能力：相对于 GPT-4 获得了 85.1% 的相对得分。. 当在科学问答（Science QA）上进行 ...</td>\n",
       "                     </tr>\n",
       "<tr>\n",
       "                       <td><a href=\"https://llava-vl.github.io/blog/2024-01-30-llava-1-6/\">LLaVA-1.6: Improved reasoning, OCR, and world knowledge</a></td>\n",
       "                       <td>Today, we are thrilled to present <b>LLaVA</b>-1.6, with improved reasoning, OCR, and world knowledge. <b>LLaVA</b>-1.6 even exceeds Gemini Pro on several benchmarks. Compared with <b>LLaVA</b>-1.5, <b>LLaVA</b>-1.6 has several improvements: Increasing the input image to 4x</td>\n",
       "                     </tr>\n",
       "<tr>\n",
       "                       <td><a href=\"https://www.microsoft.com/en-us/research/project/llava-large-language-and-vision-assistant/\">LLaVA: Large Language and Vision Assistant - Microsoft Research</a></td>\n",
       "                       <td><b>LLaVA</b> is an open-source project, collaborating with research community to advance the state-of-the-art in AI. <b>LLaVA</b> represents the first end-to-end trained large multimodal model (LMM) that achieves impressive chat capabilities mimicking spirits of the multimodal GPT-4.</td>\n",
       "                     </tr>\n",
       "<tr>\n",
       "                       <td><a href=\"https://llava-vl.github.io/llava-interactive/\">LLaVA-Interactive</a></td>\n",
       "                       <td><b>LLaVA</b>-Interactive is a system-level synergy of the inference stages of three models, without additional model training. It is surprisingly cheap to build. Checkout our code release on GitHub. For better demo experience, please play <b>LLaVA</b>-Interactive in a seperate tab by clicking me.</td>\n",
       "                     </tr>\n",
       "<tr>\n",
       "                       <td><a href=\"https://arxiv.org/abs/2304.08485\">[2304.08485] Visual Instruction Tuning - arXiv.org</a></td>\n",
       "                       <td>By instruction tuning on such generated data, we introduce <b>LLaVA</b>: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early</td>\n",
       "                     </tr>\n",
       "<tr>\n",
       "                       <td><a href=\"https://github.com/haotian-liu/LLaVA/releases\">Releases · haotian-liu/LLaVA · GitHub</a></td>\n",
       "                       <td>Release v1.1.0. 🔥 <b>LLaVA</b>-1.5 is out! This release supports <b>LLaVA</b>-1.5 model inference and serving. We will release the training scripts, data, and evaluation scripts on benchmarks in the coming week.</td>\n",
       "                     </tr>\n",
       "<tr>\n",
       "                       <td><a href=\"https://github.com/haotian-liu/LLaVA/blob/main/README.md\">LLaVA/README.md at main · haotian-liu/LLaVA · GitHub</a></td>\n",
       "                       <td>[NeurIPS&#39;23 Oral] Visual Instruction Tuning (<b>LLaVA</b>) built towards GPT-4V level capabilities and beyond. - <b>LLaVA</b>/README.md at main · haotian-liu/<b>LLaVA</b></td>\n",
       "                     </tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "rows = \"\\n\".join([\"\"\"<tr>\n",
    "                       <td><a href=\\\"{0}\\\">{1}</a></td>\n",
    "                       <td>{2}</td>\n",
    "                     </tr>\"\"\".format(v[\"url\"], v[\"name\"], v[\"snippet\"])\n",
    "                  for v in search_results[\"webPages\"][\"value\"]])\n",
    "HTML(\"<table>{0}</table>\".format(rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNg5Jz6VfBt+1bKMOC2VI7z",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
