{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd73e766",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "input your Google API and test web search\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0ea24df-ef76-40a5-9d68-52fef825d7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your search query:  llava\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def google_search(query):\n",
    "    api_key = '**************************************'\n",
    "    cx = '**********************************'\n",
    "    url = 'https://www.googleapis.com/customsearch/v1'\n",
    "    params = {'key': api_key, 'cx': cx, 'q': query}\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    return data['items']\n",
    "\n",
    "query = input('Enter your search query: ')\n",
    "search_results = google_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ad27ded-c9af-43d3-93b4-a67b4e78c825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr>\n",
       "                       <td><a href=\"https://llava-vl.github.io/\">LLaVA</a></td>\n",
       "                       <td>LLaVA Model. We introduce LLaVA (Large Language-and-Vision Assistant) , an end-to-end trained large multimodal model that connects a vision encoder and LLM for ...</td>\n",
       "                     </tr>\n",
       "<tr>\n",
       "                       <td><a href=\"https://github.com/haotian-liu/LLaVA\">haotian-liu/LLaVA: [NeurIPS'23 Oral] Visual Instruction ... - GitHub</a></td>\n",
       "                       <td>Chat about images using LLaVA without the need of Gradio interface. It also supports multiple GPUs, 4-bit and 8-bit quantized inference. With 4-bit quantization ...</td>\n",
       "                     </tr>\n",
       "<tr>\n",
       "                       <td><a href=\"https://arxiv.org/abs/2304.08485\">Visual Instruction Tuning</a></td>\n",
       "                       <td>Apr 17, 2023 ... Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal ...</td>\n",
       "                     </tr>\n",
       "<tr>\n",
       "                       <td><a href=\"https://llava.hliu.cc/\">LLaVA</a></td>\n",
       "                       <td>The service is a research preview intended for non-commercial use only, subject to the model License of LLaMA, Terms of Use of the data generated by OpenAI, and ...</td>\n",
       "                     </tr>\n",
       "<tr>\n",
       "                       <td><a href=\"https://www.microsoft.com/en-us/research/project/llava-large-language-and-vision-assistant/\">LLaVA: Large Language and Vision Assistant - Microsoft Research</a></td>\n",
       "                       <td>LLaVA is an open-source project, collaborating with research community to advance the state-of-the-art in AI. LLaVA represents the first end-to-end trained ...</td>\n",
       "                     </tr>\n",
       "<tr>\n",
       "                       <td><a href=\"https://www.reddit.com/r/LocalLLaMA/comments/18ul9ij/image_descriptions_with_llava/\">Image descriptions with LLAVA : r/LocalLLaMA</a></td>\n",
       "                       <td>Dec 30, 2023 ... Image descriptions with LLAVA ... Here is a \"good\" label: The image features a comic strip with two main characters: an angel and a runner. The ...</td>\n",
       "                     </tr>\n",
       "<tr>\n",
       "                       <td><a href=\"https://huggingface.co/docs/transformers/en/model_doc/llava\">LLaVa</a></td>\n",
       "                       <td>LLaVa is an open-source chatbot trained by fine-tuning LlamA/Vicuna on GPT-generated multimodal instruction-following data. It is an auto-regressive language ...</td>\n",
       "                     </tr>\n",
       "<tr>\n",
       "                       <td><a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1agrxnz/llamacpp_experimental_llava_16_quants_34b_and/\">[llama.cpp] Experimental LLaVA 1.6 Quants (34B and Mistral 7B) : r ...</a></td>\n",
       "                       <td>Feb 2, 2024 ... [llama.cpp] Experimental LLaVA 1.6 Quants (34B and Mistral 7B) ... They were prepared through this hacky script and is likely missing some of the ...</td>\n",
       "                     </tr>\n",
       "<tr>\n",
       "                       <td><a href=\"https://discuss.huggingface.co/t/how-to-use-llava-with-huggingface/52315\">How to use llava with huggingface - Transformers - Hugging Face ...</a></td>\n",
       "                       <td>Aug 27, 2023 ... Create a Visual Chatbot on AWS EC2 with LLaVA-1.5 and Runhouse. Get started with multimodal conversational models using the open-source LLaVA- ...</td>\n",
       "                     </tr>\n",
       "<tr>\n",
       "                       <td><a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1amwsk2/has_anyone_encountered_issues_with_llava_16/\">Has Anyone Encountered Issues with LLaVA 1.6 Models on Ollama ...</a></td>\n",
       "                       <td>Feb 9, 2024 ... Make sure you are running latest version of LLaVA, for me it runs superfast, even tho sometimes server can crash, when the response requires ...</td>\n",
       "                     </tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "rows = \"\\n\".join([\"\"\"<tr>\n",
    "                       <td><a href=\\\"{0}\\\">{1}</a></td>\n",
    "                       <td>{2}</td>\n",
    "                     </tr>\"\"\".format(v[\"link\"], v[\"title\"], v[\"snippet\"])\n",
    "                  for v in search_results])\n",
    "HTML(\"<table>{0}</table>\".format(rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58aea72-a4a8-4131-b03a-e86d5d7707c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
